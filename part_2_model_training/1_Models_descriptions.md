  **Сбор и разметка данных**  
  Для тренировки модели по подсчету количества бластоспор на фотографиях первым этапом был сбор и разметка данных. При проведении исследований все подсчеты количества бластоспор выполнялись на оборудовании без фотографирования, таким образом, изначально не было даже набора фотографий для разметки и решения задачи. В этом и заключалась основная сложность для выбора данной задачи в качестве финального проекта (Не было понимания количества времени, необходимого для сбора и разметки фотографий). В соответствии с [описанием](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results), приведенным в инструкции для модели YOLOv5 для получения наилучших результатов тренировки модели YOLOv5 рекомендуется использовать не менее 1500 изображений с количеством размеченных объектов не менее 10000.

  При выборе задачи оценивалось время, необходимое на фотографирование и разметку. Специфика и конструкция устройства забора проб для подсчета количества бластоспор позволяет получить до 50 различных фотографий за один забор. В исследованиях параллельно работают 2 биореактора. С учетом наличия у научных сотрудников текущих задач, в день было возможно провести 2 забора проб из каждого биореактора и сделать 200 фотографий. За 3 дня было получено 500 различных фотографий, что уже достаточно для создания прототипа модели и основы работающего сервиса при наличии разметки фотографий.

  Для выполнения разметки фотографий разработаны и выложены на github различные приложения. В своей задаче я использовал библиотеку **[label-studio](https://github.com/heartexlabs/label-studio)** . Для оценки времени, необходимого для разметки фотографий, мной была проведена разметка 10 фотографий с помощью label-studio: затраченное время для 10 фотографий составило около 1 ч 10 мин (в среднем разметка одной фотографии заняла около 7 мин, количество размечаемых объектов варьировалось от 50 до 180). Также необходимо учесть, что после разметки 30 фотографий, скорость разметки сильно падает (практика показала, что на 30 фотографий уходило около 4 часов с учетом небольших перерывов, в выходной день меня хватало на разметку 50 фотографий). Таким образом, для разметки 500 фотографий мне бы потребовалось около 10 дней без учета наличия иных задач, помимо финального проекта, что довольно много. В связи с этим было решено распараллелить работу между еще двумя людьми и освободить время на работу с моделью для обучения. Такие люди были найдены, но, чтобы они проводили разметку необходимо было установить label-studio к ним на компьютеры, работающие на Windows и объяснить, как производится разметка.
Небольшое отступление: когда регулярно работаешь и устанавливаешь приложения с github, то установка label-studio не представляет каких-либо затруднений. У меня на компьютере стоит вторая операционная система Linux Ubuntu, установка и запуск label-studio на Linux также не вызвала затруднений (состоит из трех строк): To install Label Studio with pip and a virtual environment, you need Python version 3.7 or later. Run the following: python3 -m venv env далее source env/bin/activate и далее python -m pip install label-studio.

  Для установки label-studio на Windоws пришлось затратить больше времени, поэтому привожу краткую инструкцию для установки label-studio на Windows:  
  Скачиваем python c официального сайта [python](https://www.python.org/) и устанавливаем (запуск exe-файла). При установке нужно поставить галочку в флажок «Добавить Python 3.x в PATH» или позже вручную добавить путь к python.exe и папке Scripts в PATH Windows (Запуск python, запуск виртуальных сред или команд pip install через Терминал Windows возможен в случае, если python добавлен в переменную среды Windows PATH). Инструкции по добавлению пути к python.exe и папке Scripts в PATH Windows можно без труда найти соответствующим запросом в интернете. Для проверки установки - запускаем терминал Windows и пишем команду python --version. Если будет возращена установленная версия python, то можно переходить к следующему шагу.
  - Создаем новое виртуальное окружение (среду) с именем "env" (имя можно выбрать любое), для этого в командной строке вводим: python -m venv env
  - Переходим в папку с созданным виртуальным окружением командой: cd env
  - Активируем виртуальное окружение командой: .\Scripts\activate (после выполнения команды в терминале отобразится: (env) C:\Users\user_name\env>)
  - Устанавливаем библиотеку label-studio командой: python -m pip install label-studio
  - Запуск программы осуществляем командой: label-studio (программа запускается на локальном сервере в http://localhost:8080)
  - После окончания работы в label-studio деактивация/выход из виртуального окружения осуществляется командой: deactivate
  - В дальнейшем для запуска программы при открытии терминала повторяем шаги 3,4,6.

  Интерфейс программы для настройки проекта интуитивно понятен: логинимся (создаем локальный профиль) -> выбираем создать проект + вводим название -> выбираем тип разметки под решаемую задачу (Object Detection with Bounding Boxes) и вводим названия объектов для разметки (в моем случае использовал краткое название: micro_orgs). Фотографии можно импортировать, как при создании проекта, так и после его создания. При проведении разметки рекомендую в настройках поставить галочку "Keep label selected after creating a region", чтобы не включать каждый раз инструмент разметки. Разметка осуществляется просто и быстро: создание прямоугольной области bounding box осуществляется нажатием и последующим отпусканием левой кнопки мыши, так, чтобы размечаемый объект попал в выделенную область.

  После установки label-studio еще на двух компьютерах, настройки проекта и объяснения особенностей работы программы, разметка 500 фотографий была выполнена в течении 1 недели (разметка 210 + 89 фотографий за 5 дней была осуществлена мной и еще одним человеком, еще 205 фотографий разметил научный сотрудник лаборатории в рабочее время за 4 дня). Общее затраченное время на разметку 504 фотографий составило 7-8 дней при наличии у меня иных задач, не связанных с проектом и с учетом работы над моделью проекта. Набор фотографий представлен в папке: images. Перечень файлов с координатами bounding boxes представлен в папке: labels.
  
  **Модель 1: Faster R-CNN**  
  Faster R-CNN была первая опробованная модель. При создании модели использовались материалы статьи https://habr.com/ru/post/579050/ с учетом того, что в проекте использовался собственный размеченный датасет. В статье также ссылаются на примеры использования готовой модели Faster R-CNN из библиотеки torchvision из **[официальной документации](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)** и **[руководства](https://debuggercafe.com/road-pothole-detection-with-pytorch-faster-rcnn-resnet50/)**. Стоит отметить, что у модели есть ограничение и в данной задаче оно существенным, поэтому рекомендуется предварительно с ним ознакомиться в конце данного описания.  
  
  В связи с тем, что исходные фотографии имеют большие размеры (2592х1944 пикселя), перед подачей в модель фотографии были предварительно уменьшены в размерах до величины 480х320 пикселя с помощью функции ***scale_image***, а также соответствующим образом преобразованы размеченные bounding box'es. Далее данные с названием изображений, классов и списки bounding box были преобразованы в DataFrame. Посмотреть размеченное изображение из dataframe можно с помощью функции ***draw_img_with_box***. Позже при анализе кода на github используемой модели было замечено, что в модель встроен блок преобразования изображения к размеру 800 пикселей, однако использование больших размеров при обучении модели без предварительного преобразования увеличивало время работы модели при обучении. Важной особенностью указанного блока в модели стала возможность использования изображения без предварительных преобразований при детектировании модели на тестовых изображениях.  
  
  Экземпляр класса *torch.utils.data.Dataset* по индексу изображения выдает на выходе изображение в формате torch.tensor и словарь target, в котором будет информация о искомом объекте: координаты рамки и класс. В конструктор класса передаются данные таблицы из DataFrame и путь до папки с изображениями.  
  Модель создается с помощью функции ***create_model***. Так как модель будет детектировать только одну модель, то внутри функции переопредляется *box_predictor* с количеством выходов - 2 (нулевой класс - фон).  
  
   Вспомогательная функция применяется к бачам при итерации по *torch.utils.data.DataLoader* (Позволяет избегать ошибок c размерностями внутри бачей).  
   
   Деление датасета на тренировочную, тестовую и валидационную выборки производятся с помощью функции *train_test_split* из библиотеки **sklearn** с зафиксированным random_state.  
   
   Далее настраиваются параметры обучения: определяется устройство, на котором будет обучаться модель, создается модель с помощью *create_model*, выбирается оптимизатор, скорость обучения и настройка изменения скорости обучения. Далее данные оборачиваются в *torch.utils.data.DataLoader* с возможностью установки размер бача (см. *train_data_loader*, *val_data_loader*). Изображения перемешиваются из тренировочной выборки перед подачей в модель.  
   
   В качестве модели использовалась модель **[fasterrcnn_resnet50_fpn](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)** из библиотеки **torchvision.models.detection**.  
   
   Функции для тренировки (***train***) и валидации модели (***val***), внутри которых все данные внутри бача переводятся на устройство, на котором будут производится расчеты. Затем полученные тензоры подаются в модель и получается словарь со значениями функций потерь. После чего считается их сумма с записью в переменную *running_loss* для отслеживания прогресса обучения. В конце каждой эпохи выводится среднее значение функций потерь. Функция валидации модели от функции тренировки отличается отсутствием расчета градиента при валидации.  
   
   При проведении обучения создаются два списка в которые сохраняются значения функций потерь после каждой эпохи на тренировке и валидации.  
   Анализ хода обучения показывает, что при обучении на 20 эпохах Loss падает, однако поведение функции потерь на валидационной выборке показывает, что модель все же начинает переобучаться и нужно поработать над настройкой модели и/или увеличение количества размеченных изображений и/или поработать с аугментацией.
   
   **Расчет метрики оценки качества работы модели**  
   При оценке метрик использовалась библиотека **[Object-Detection-Metrics](https://github.com/rafaelpadilla/Object-Detection-Metrics)** и соответствующие инструкции. В качестве метрики для оценки качества предсказания модели использовалась метрика, предложенная в соревновании **[PascalVOC](http://host.robots.ox.ac.uk/pascal/VOC/)**. Данная метрика основана на интерполированной метрике average precision (АР), которая рассчитывается с использованием площади под интерполированной кривой Precision x Recall. Более подробно можно узнать в используемой библиотеке [Object-Detection-Metrics](https://github.com/rafaelpadilla/Object-Detection-Metrics) и [источнике](https://medium.com/@vijayshankerdubey550/evaluation-metrics-for-object-detection-algorithms-b0d6489879f3).  
   
   Для использования метрики ***PascalVOC*** в ноутбуке импортируются функции из библиотеки **object_detection_metrics**, создаются папки (*grnd_truths/*, *detections/*), в которых будут помещаться вспомогательные файлы координат предсказанных и размеченных bounding boxes. Вспомогательные файлы содаются функцией ***write_boxes_for_metrics***, структура вспомогательных файлов определена в соответствии с инструкцией, приведенной к библиотеке **[Object-Detection-Metrics](https://github.com/rafaelpadilla/Object-Detection-Metrics)**.  
   
   Функция ***draw_predict*** создает в папках (*grnd_truths/*, *detections/*) файлы с координатами размеченных и предсказанных bounding boxes (с использованием функции ***write_boxes_for_metrics***) для последующего расчета метрики по индексу данных, представленных в датафрейме. В функции при предсказании bounding boxes применен алгоритм non-maximum suppression, который реализован в библиотеке torchvision. Он объединяет похожие рамки на основе их взаимного пересечения, которые были предсказаны моделью. Дополнительно функция выводит на один рисунок изображение, размеченные и предсказанные bounding boxes для визуального анализа результатов работы модели.  
   
   Функция ***getBoundingBoxes*** предназначена для преобразования txt файлов с координатами bounding boxes (ground truth and detections) в формат для оценки метрики PascalVOC с помощью функции ***GetPascalVOCMetrics*** описанной в классе **Evaluator**. Подробная инструкция приведена **[в описании](https://github.com/rafaelpadilla/Object-Detection-Metrics/tree/master/samples/sample_2)**.  
    
   Ноутбук **[2_1_Faster_RCNN_v3_resnet_50.ipynb](https://github.com/ostrebko/skf_final_project/blob/main/part_2_model_training/2_1_Faster_RCNN_v3_resnet_50.ipynb)** представлен в проекте в папке **part_2_model_training**, а также предварительные результаты обучения модели представлены в **[ноутбуке в Google Colab](https://colab.research.google.com/drive/1hdh-9ChEjSyOrZg6QcOOW6wMiln0gGnw?usp=sharing)**.   
   
  **Анализ работы модели fasterrcnn_resnet50_fpn**  
  Предварительный анализ работы модели сделан на основании 10 фотографий, представленных в тестовых (валидационных) данных, не участвующих в тренировке модели, и представлен в конце [ноутбука](https://colab.research.google.com/drive/1zLgh1fKG5nWR_Tb4mZva-eh9QME1OJzz?usp=sharing). Наряду с "очень хорошей" предсказанием положения бластоспор моделью (точность 87%-97%, см. индексы фото в валидационном датасете 50, 75, 5, 25), встречаются и фотографии, на которых модель работает "довольно плохо" (точность 64-72%, см. индексы фото в валидационном датасете 40, 79, 78, 33), так и "очень плохо" (49%, 57%, см. индексы фото в валидационном датасете 77, 13). Столь большой разброс в определении положения бластоспор моделью связывается со следующими причинами: 
  - тренировочный датасет состоит из 320 фотографий, что довольно мало для обучения;  
  - поведение Loss на графике ([см. ноутбук](https://colab.research.google.com/drive/1zLgh1fKG5nWR_Tb4mZva-eh9QME1OJzz?usp=sharing)), свидетельствует о том, что модель начала переобучаться;  
  - неоднозначное определение бластоспор при разметке (на фотографиях встречаются "мусор", который бывает похож на бластоспоры по форме; на фотографиях встречаются "артефакты" фотосъемки (очень бледные замкнутые контуры), при этом сами бластоспоры могут не иметь четких границ и быть также блеклыми;   предварительная разметка проводилась 3 разными людьми, каждый их которых в спорных моментах мог либо отнести при разметке анализируемый объект к бластоспорам либо не отнести; наличие скоплений ("слипшиеся группы") бластоспор, не всегда позволяющих однозначно определить границы при разметке);  
  - лучшие результаты работы модели были получены, для фотографий, на которых: не было "мусора", границы бластоспор были яркие и четкие, отсуствовали скопления ("слипшиеся группы") бластоспор. Следует отметить, что такие фотографии при ручной разметке требовали меньше времени по сравнению с остальными фотографиями.
   
  **Работа с разными backbone**
  В связи с тем, что модели с разными структурами могут давать разный результат при детектировании объектов, было опробована работа моделей с разными блоками backbone (часть сети, отвечающая за построение признаков изображения). Рассматривались модели со следующими блоками backbone, которые можно импортировать с помощью функции resnet_fpn_backbone с помощью библиотеки [torchvision.models.detection.backbone_utils](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/backbone_utils.py): *[resnet_18, resnet_34, resnet_50 (для анализа сравнения с уже готовой моделью fasterrcnn_resnet50_fpn для импорта), resnet_101, resnet_152](https://drive.google.com/drive/folders/1WxDmnt1f6AXFxA7wRc1T3xsrAji5bDkV?usp=sharing)*. Для решения данного вопроса помог комментарий описанный в конце [ссылки](https://stackoverflow.com/questions/58362892/resnet-18-as-backbone-in-faster-r-cnn). Также были проанализированы работа моделей *fasterrcnn_mobilenet_v3_large_fpn*, *fasterrcnn_mobilenet_v3_large_320_fpn* из библиотеки **torchvision.models.detection**. Анализ проводился для схожих параметров, таких как: количество эпох - 20, параметры optimizer'a и sheduler'a. С единственной разницей, что для "более тяжелых" моделей batch_size уменьшался, чтобы хватило в памяти GPU Google Colab.  
  
  **Результаты анализа:**  
  Хуже всего детектирование проводила модель *fasterrcnn_mobilenet_v3_large_320_fpn*, что не удивительно, так как в [описании](https://pytorch.org/vision/stable/_modules/torchvision/models/detection/faster_rcnn.html) указано: "Constructs a low resolution Faster R-CNN model with a MobileNetV3-Large FPN backbone tunned for mobile use-cases". Данная модель в дальнейшем была сразу исключена из рассмотрения.  
  Сравнение остальных моделей проводился на валидационном наборе изображений, которые не участвовали в тренировке модели. В целом работа моделей показала, что работа моделей по качеству метрики PascalVOC (описание см. выше) похожи, при этом результаты предсказания одной модели на некоторой выбранной фотографии могут быть лучше результатов предсказания второй модели, а на другой фотографии вторая модель может показать результат лучше, чем первая модель. На основании этого результаты качества предсказания модели метрикой PascalVOC для быстрого сравнения были усреднены по всем фотографиям. Такая грубая оценка позволяет сразу отсеять модели, которые работают хуже остальных и сосредоточиться на нескольких лучших (была отсеяна модель *fasterrcnn_mobilenet_v3_large_fpn*). Интересный вывод был получен сравнении усредненных по фотографиям метрик PascalVOC для остальных моделей (*resnet_18, resnet_34, resnet_50, resnet_101, resnet_152*), которое изменялось от 0.6921 до 0,7070. Столь малый разброс может быть связан с тем, что объекты для детектирования не сложными по своей структуре объектами, на основании чег даже более простая модель с backbone *resnet_18* по усредненной метрике сравнима с *resnet_152*, а это значит, что можно использовать более простую модель для детектирования объектов, подобным бластоспорам.  
  Результаты работы моделей, в т.ч. с отображением результатов предсказания и разметки, представлены в [ноутбуках](https://drive.google.com/drive/folders/1WxDmnt1f6AXFxA7wRc1T3xsrAji5bDkV?usp=sharing). Все предсказанные значения метрики PascalVOC по каждой модели были сохранены в отдельную папку, с последующим усреднением и сравнением, код представлен в [ноутбуке](https://colab.research.google.com/drive/1cWEz6gTIkLCP42F5s_qOiOrN7X_1DC6s?usp=sharing).  
  При анализе результатов предсказания были получены существенные ограничения моделей (см. ниже) по детекции бластоспор.  
    
  **Ограничения в предсказании модели**  
  При анализе результатов предсказания моделей, в том числе при оценке бизнес-метрики было обнаружено, что количество предсказанных объектов не превышает 100 бластоспор. Анализ таких фотографий, показал, что модель детектирует не все бластоспоры (всегда имелись не предсказанные объекты). Даже, если предположить, что модель при предсказании может ошибаться, все же количество детектированных объектов ни в одном случае не превысило 100 шт. В **[источнике](https://stackoverflow.com/questions/65499420/pytorch-torchvision-how-to-increase-limit-of-detectable-objects)** также был отмечен лимит 100 объектов при детектировании объектов с помощью fasterrcnn_resnet50_fpn. Причины не были указаны, но в качестве решения предлагалось использование модели YOLOv5. Предварительный анализ кода на **[github](https://github.com/ultralytics/yolov5/blob/master/detect.py)** показал, что количество предсказываемых объектов может доходить до 1000. Так как ограничение в 100 бластоспор являлось существенным для задачи данного проекта, было решено перейти на модель **YOLOv5** (Ноутбук для тренировки модели приведен в файле *3_1_YOLO5_Model.ipynb*). Стоит отметить, что использование вышеуказанных моделей Faster R-CNN возможно в задачах, где не требуется детектирование столь большого количества объектов.  
  
  **Способы улучшения качества работы модели**  
  Ниже приведены возможные способы улучшения качества рассмотренных моделей fasterrcnn для задач, в которых количество детектируемых объектов не превышает 100:
  - увеличение количества размеченных фотографий до 1500-2000, в т.ч. с учетом автоматизированной разметки данных;
  - добавление аугментации изображений при обучении модели;
  - корректировка размеченных фотографий (для сложных объектов - постановка четких критериев определения разметки объекта) с целью исключения неверной разметки данных;
  - более точная настройка параметров модели и порогов определения bounding boxes; 
  - обучение на большем количестве эпох (до 300-500 эпох);
  - использование других моделей SOTA для обучения.

  **Модель 2: YOLOv5**  
  Модель YOLO основана на разбиении картинки равномерной сеткой с последующим предсказанием классификации и регрессии для набора потенциальных якорных боксов. Подробнее про принципы работы модели можно посмотреть в данном [источнике](https://medium.com/deepsystems-ru/yolo-you-only-look-once-%D0%BE%D0%B1%D0%B7%D0%BE%D1%80-%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D1%8B-482ac4d92df6).  
  Для обучения была выбрана модель **YOLOv5** от разработчика **[ultralytics](https://ultralytics.com/)**. YOLOv5 - это семейство архитектур и моделей обнаружения объектов, предварительно обученных на наборе данных COCO и реализовано с использованием фреймворка Pytorch. Документация представлена [по ссылке](https://docs.ultralytics.com/), проект реализован на **[github](https://github.com/ultralytics/yolov5)** и содержит подробные инструкции использования данной библиотеки для обучения модели на данных, которые не содержатся в датасете COCO ("Custom Data"), инструкции по заморезке слоев при transfer learning, ансамблированию и TTA во время тестирования и инференса модели и т.п. 
  Для обучения модели использовалась инструкции ["Train Custom Data"](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data) с использованием ноутбука в [Google Colab](https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb). В указаннной инструкции для более простого приведения размеченного датасета к формату данных для обучения и тестирования модели предлагается использовать **[Roboflow](https://roboflow.com/?ref=ultralytics)**. Более подробно использование Roboflow представлено в видео на [странице разработчика](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/?ref=ultralytics). Однако, бесплатная версия использования RoboFlow доступна, если размеченные данные сделать публичными. В связи с отсуствием договоренности по данному вопросу, а также припиской к письмам при получении фото о конфиденциальности данных было решено самостоятельно подготовить данные для обучения модели YOLOv5.  
  Ноутбук **[3_1_YOLO5_Model.ipynb](https://github.com/ostrebko/skf_final_project/blob/main/part_2_model_training/3_1_YOLO5_Model.ipynb)** с подготовкой данных, тренировкой и предсказанием модели представлен в проекте в папке **part_2_model_training**.  
  Для самостоятельной подготовки данных использовались следующие источники: **[источник 1](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#11-create-datasetyaml)**, **[источник 2](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/?ref=ultralytics)**, **[источник 3](https://vc.ru/newtechaudit/326571-ispolzovanie-yolov5-dlya-zadachi-detekcii)**. Из [источника 1](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#11-create-datasetyaml) стало ясно, что данные для тренировки модели можно подготовить самостоятельно, при этом описание, на мой взгляд, оказалось довольно запутанным и было решено обратиться к иным источникам. В видео [источника 2](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/?ref=ultralytics) было изучено, каким образом организованы данные для обучения модели с помощью Roboflow, но при этом в нем не было показано, как организуются данные в файле data.yaml. На примере [источника 3](https://vc.ru/newtechaudit/326571-ispolzovanie-yolov5-dlya-zadachi-detekcii) был создан файл data.yaml, а также стала ясна структура организации папок train и valid.  
  YOLOv5 принимает на вход изображение квадратной формы, поэтому для преобразования фотографий была написана функция *scale_image_label*, которая приводит изображение к квадратному виду заданного размера в пикселях (для обучения исходные картинки приводились к размеру 640х640), копирую преобразованные изображения в отдельную папку (чтобы была возможность поробовать обучить модель на изображениях разных размеров). При разметке изображений в **label-studio** (описано в подразделе "Сбор и разметка данных") координаты bounding box экспортировались в формат YOLO, который соответствует принимаемым YOLOv5 данным, в связи с чем дополнительные преобразования координат не потребовались. В функция *scale_image_label* реализовано копирование файлов с координатами bounding box в отдельную папку после изменения размеров изображения, что также является дополнительной проверкой наличия файла с координатами соответствующему изображению.  
  После преобразования данные делятся на тренировочные, валидационные и тестовые данные в пропорции примерно 70% - 20% - 10%, что в подготовленном наборе данных соответствует 492шт.- 141шт. - 71шт. изображений. Деление данных происходит по индексам списков названий файлов изображений и файлов с координатами bounding box после их предварительной сортировки с помощью функции *train_test_split* библиотеки *sklearn* при зафиксированном random seed = 123.  
  Далее с помощью библиотеки *shutil* файлы изображений и координат перемещаются по папкам, структура которых принимается моделью YOLOv5 для обучения и предсказания. На мой взгляд, структура папок наиболее понятно описана в [источнике 3](https://vc.ru/newtechaudit/326571-ispolzovanie-yolov5-dlya-zadachi-detekcii).  
  После подготоки данных обучение модели происходит с помощью инструкций и ноутбука [Google Colab](https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb):  
  1. Датасет мы подготовили заранее, поэтому шаг 2 из ноутбука можно пропустить.
  2. Клонируется репозиторий github и устанавливаются зависимости.
  3. Для обучения была выбрана модель **YOLOv5s** с соответствующими ей весами *yolov5s.pt*, предобученными на COCO128 в связи с тем, что бластоспоры не имеют сложной формы и структуры и ожидается, что качество предсказания более простой модели будет сравнимо с более сложными моделями. Перед запуском тренировки модели (запуск train.py осуществляется с помощью python в строке ноутбука) необходимо выставить параметры обучения:
     - *img*: размер входного изображения (выбрано 640);
     - *batch*: размер батча (выбрано 16);
     - *epochs*: количество эпох для обучения модели (выбрано 300);
     - *data*: расположение файла data.yaml с описанием конфигурации данных (указан соответствующий путь);
     - *weights*: путь и название файла весов предобученной на датасете COCO модели для transfer learning (выбрано yolov5s.pt);
     - *cache*: кешерование изображений для более быстрой тренировки модели (выбор осуществляется без параметров и в таком случае значение определяется, как True, если параметр не указан, то параметр принимает значение False);
     - *workers*: количество параллельных потоков, на которых запускаем обучение для более быстрой тренировки модели (выбрано 2).  
  Дополнительные параметры тренировки модели можно изучить в файле *[train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)* (см. функцию *parse_opt*).  
  После обучения модели веса последней эпохи (last) и лучшей эпохи (best) сохраняются в папку runs/train/exp/weights/.  
  Все результаты сохраняются в папку runs/train/exp (при повторном обучении сохранение идет в папки exp2, exp3, exp4).  
  Оценка качества работы модели осуществляется после тренировки модели на валидационных данных. Результаты сохраняются в папку runs/train/exp.  
  4. Перед запуском инференса (запуск detect.py осуществляется с помощью python в строке ноутбука) c весами модели, полученными при тренировке модели, необходимо выставить параметры:
     - *weights*: путь и название файла весов обученной модели с лучшими результатами.
     - *img*: размер входного изображения (выбрано 640);  
     - *conf*: значение порога threshold (выбрано 0.1);  
     - *source*: путь к тестовым изображениям  (file/dir/URL/glob);  
     - *line-thickness*: толщина линий прорисовки предсказанных bounding box (в пикселях, выбрано 1);  
     - *hide-labels*: скрыть названия классов (без выбора параметров);  
     - *save-txt*: сохранить пресказанные результаты в \*.txt (без выбора параметров);  
  Все картинки сохраняются в папку runs/detect/exp, файлы txt с пресказанными результатами bounding box - в папку runs/detect/exp/labels.  
  В ноутбуке также реализовано скачивание картинок, файлов txt с пресказанными результатами bounding box, а также вывод всех пресказанных bounding box на тестовых изображениях и сохранение файла с весами лучшей модели (best.pt) на локальный компьютер.  
  5. Дополнительно рассчитывается среднее число предсказанных бластоспор на тестовых значениях и (при необходимости) оценивается время, необходимое для предсказания модели на CPU.  

  **Анализ работы модели YOLOv5**  
  
