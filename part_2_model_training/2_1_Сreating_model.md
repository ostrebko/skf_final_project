  **Модель Faster-RCNN**  
  При создании модели использовались материалы статьи https://habr.com/ru/post/579050/ с учетом того, что в проекте использовался собственный размеченный датасет. В статье также ссылаются на примеры использования готовой модели Faster-RCNN из библиотеки torchvision из **[официальной документации](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)** и **[руководства](https://debuggercafe.com/road-pothole-detection-with-pytorch-faster-rcnn-resnet50/)**.  
  
  В связи с тем, что исходные фотографии имеют большие размеры (2592х1944 пикселя), перед подачей в модель фотографии были предварительно уменьшены в размерах до величины 480х320 пикселя с помощью функции ***scale_image***, а также соответствующим образом преобразованы размеченные bounding box'es. Далее данные с названием изображений, классов и списки bounding box были преобразованы в DataFrame. Посмотреть размеченное изображение из dataframe можно с помощью функции ***draw_img_with_box***.  
  
  Экземпляр класса *torch.utils.data.Dataset* по индексу изображения выдает на выходе изображение в формате torch.tensor и словарь target, в котором будет информация о искомом объекте: координаты рамки и класс. В конструктор класса передаются данные таблицы из DataFrame и путь до папки с изображениями.  
  Модель создается с помощью функции ***create_model***. Так как модель будет детектировать только одну модель, то внутри функции переопредляется *box_predictor* с количеством выходов - 2 (нулевой класс - фон).  
  
   Вспомогательная функция применяется к бачам при итерации по *torch.utils.data.DataLoader* (Позволяет избегать ошибок c размерностями внутри бачей).  
   
   Деление датасета на тренировочную, тестовую и валидационную выборки производятся с помощью функции *train_test_split* из библиотеки **sklearn** с зафиксированным random_state.  
   
   Далее настраиваются параметры обучения: определяется устройство, на котором будет обучаться модель, создается модель с помощью *create_model*, выбирается оптимизатор, скорость обучения и настройка изменения скорости обучения. Далее данные оборачиваются в *torch.utils.data.DataLoader* с возможностью установки размер бача (см. *train_data_loader*, *val_data_loader*). Изображения перемешиваются из тренировочной выборки перед подачей в модель.  
   
   Функции для тренировки (***train***) и валидации модели (***val***), внутри которых все данные внутри бача переводятся на устройство, на котором будут производится расчеты. Затем полученные тензоры подаются в модель и получается словарь со значениями функций потерь. После чего считается их сумма с записью в переменную *running_loss* для отслеживания прогресса обучения. В конце каждой эпохи выводится среднее значение функций потерь. Функция валидации модели от функции тренировки отличается отсутствием расчета градиента при валидации.  
   
   При проведении обучения создаются два списка в которые сохраняются значения функций потерь после каждой эпохи на тренировке и валидации.  
   Анализ хода обучения показывает, что при обучении на 20 эпохах Loss падает, однако поведение функции потерь на валидационной выборке показывает, что модель все же начинает переобучаться и нужно поработать над настройкой модели и/или увеличение количества размеченных изображений и/или поработать с аугментацией.
   
   **Расчет метрики оценки качества работы модели**  
   При оценке метрик использовалась библиотека **[Object-Detection-Metrics](https://github.com/rafaelpadilla/Object-Detection-Metrics)** и соответствующие инструкции. В качестве метрики для оценки качества предсказания модели использовалась метрика, предложенная в соревновании **[PascalVOC](http://host.robots.ox.ac.uk/pascal/VOC/)**. Данная метрика основана на интерполированной метрике average precision (АР), которая рассчитывается с использованием площади под интерполированной кривой Precision x Recall. Более подробно можно узнать в используемой библиотеке [Object-Detection-Metrics](https://github.com/rafaelpadilla/Object-Detection-Metrics) и [источнике](https://medium.com/@vijayshankerdubey550/evaluation-metrics-for-object-detection-algorithms-b0d6489879f3).  
   
   Для использования метрики ***PascalVOC*** в ноутбуке импортируются функции из библиотеки **object_detection_metrics**, создаются папки (*grnd_truths/*, *detections/*), в которых будут помещаться вспомогательные файлы координат предсказанных и размеченных bounding boxes. Вспомогательные файлы содаются функцией ***write_boxes_for_metrics***, структура вспомогательных файлов определена в соответствии с инструкцией, приведенной к библиотеке **[Object-Detection-Metrics](https://github.com/rafaelpadilla/Object-Detection-Metrics)**.  
   
   Функция ***draw_predict*** создает в папках (*grnd_truths/*, *detections/*) файлы с координатами размеченных и предсказанных bounding boxes (с использованием функции ***write_boxes_for_metrics***) для последующего расчета метрики по индексу данных, представленных в датафрейме. В функции при предсказании bounding boxes применен алгоритм non-maximum suppression, который реализован в библиотеке torchvision. Он объединяет похожие рамки на основе их взаимного пересечения, которые были предсказаны моделью. Дополнительно функция выводит на один рисунок изображение, размеченные и предсказанные bounding boxes для визуального анализа результатов работы модели.  
   
   Функция ***getBoundingBoxes*** предназначена для преобразования txt файлов с координатами bounding boxes (ground truth and detections) в формат для оценки метрики PascalVOC с помощью функции ***GetPascalVOCMetrics*** описанной в классе **Evaluator**. Подробная инструкция приведена **[в описании](https://github.com/rafaelpadilla/Object-Detection-Metrics/tree/master/samples/sample_2)**.  
    
   Ноутбук **2_2_Faster_RCNN_blastospores_v2.ipynb** представлен в проекте в папке **part_2_model_training**, а также предварительные результаты обучения модели представлены в [ноутбуке в Google Colab](https://colab.research.google.com/drive/1zLgh1fKG5nWR_Tb4mZva-eh9QME1OJzz?usp=sharing).
   
 **Анализ работы модели**  
 Предварительный анализ работы модели сделан на основании 10 фотографий, представленных в тестовых (валидационных) данных, не участвующих в тренировке модели, и представлен в конце [ноутбука](https://colab.research.google.com/drive/1zLgh1fKG5nWR_Tb4mZva-eh9QME1OJzz?usp=sharing). Наряду с "очень хорошей" предсказанием положения бластоспор моделью (точность 87%-97%, см. индексы фото в валидационном датасете 50, 75, 5, 25), встречаются и фотографии, на которых модель работает "довольно плохо" (точность 64-72%, см. индексы фото в валидационном датасете 40, 79, 78, 33), так и "очень плохо" (49%, 57%, см. индексы фото в валидационном датасете 77, 13). Столь большой разброс в определении положения бластоспор моделью связывается со следующими причинами: 
- тренировочный датасет состоит из 320 фотографий, что довольно мало для обучения;  
- поведение Loss на графике ([см. ноутбук](https://colab.research.google.com/drive/1zLgh1fKG5nWR_Tb4mZva-eh9QME1OJzz?usp=sharing)), свидетельствует о том, что модель начала переобучаться;  
- неоднозначное определение бластоспор при разметке (на фотографиях встречаются "мусор", который бывает похож на бластоспоры по форме; на фотографиях встречаются "артефакты" фотосъемки (очень бледные замкнутые контуры), при этом сами бластоспоры могут не иметь четких границ и быть также блеклыми;   предварительная разметка проводилась 3 разными людьми, каждый их которых в спорных моментах мог либо отнести при разметке анализируемый объект к бластоспорам либо не отнести; наличие скоплений ("слипшиеся группы") бластоспор, не всегда позволяющих однозначно определить границы при разметке);  
- лучшие результаты работы модели были получены, для фотографий, на которых: не было "мусора", границы бластоспор были яркие и четкие, отсуствовали скопления ("слипшиеся группы") бластоспор. Следует отметить, что такие фотографии при ручной разметке требовали меньше времени по сравнению с остальными фотографиями.  

**Способы улучшения качества работы модели**  
- увеличение количества размеченных фотографий до 1500-2000, в т.ч. с учетом автоматизированной разметки данных;
- использование аугментации изображений при обучении модели;
- корректировка размеченных фотографий своместно с научным сотрудником (постановка четких критериев определения бластоспор) с целью исключения неверной разметки данных (исключить из разметки "мусор", артефакты съемки, скопления ("слипшиеся группы") бластоспор);
- более точная настройка параметров модели и порогов определения bounding boxes; 
- обучение на большем количестве эпох (до 300-500 эпох);
- использование других моделей SOTA для обучения, например YOLO5.
